{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running it only for one specific bug and test file to check the behaviour\n",
    "bug_report_file = \"/Users/lorenapacheco/Concordia/Masters/BugReportsMining/bug-reports/commons-cli/commons-cli-CLI-133.json\"\n",
    "defects4j_id = \"Cli-5\"\n",
    "buggy_commit = \"2b0a94aee899d9e7d855c402ad40eb4e318f46e7\"\n",
    "repo_folder = \"/Users/lorenapacheco/Concordia/Masters/open_source_repos_being_studied/commons-cli\"\n",
    "test_file = \"/Users/lorenapacheco/Concordia/Masters/open_source_repos_being_studied/commons-cli/src/test/org/apache/commons/cli/ValuesTest.java\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLKT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def preprocess(file):\n",
    "    return subprocess.check_output(['java', '-jar', '../lib/preprocessing.jar', file]).decode(\"utf-8\")\n",
    "\n",
    "def stop_words_removal(voc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w.lower() for w in voc if not w.lower() in stop_words and not is_symbol(w)]\n",
    "    return tokens\n",
    "\n",
    "def is_symbol(word):\n",
    "    return re.search('\\w+', word) == None\n",
    "\n",
    "def load_java_keywords():\n",
    "    file = os.path.join(os.path.abspath(''), '/Users/lorenapacheco/Concordia/Masters/BugReportsMining/textualSimilarity/java_keywords.txt')\n",
    "    f = open(file, \"r\")\n",
    "    if f.mode == \"r\":\n",
    "        contents = f.read()\n",
    "        return contents.split('\\n')\n",
    "    return None\n",
    "\n",
    "def java_keywords_removal(voc):\n",
    "    java_keywords = load_java_keywords()\n",
    "    if java_keywords == None:\n",
    "        print('java_keywords.txt unavailable')\n",
    "        return\n",
    "    return [w for w in voc if not w in java_keywords]\n",
    "\n",
    "def stemming(voc):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in voc]\n",
    "\n",
    "def tokenize(source):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(source)\n",
    "\n",
    "def code_to_corpus(source):\n",
    "    package = source.splitlines()[-1]\n",
    "    voc = tokenize(source.replace('_', ' '))\n",
    "    voc = camel_case_split(voc)\n",
    "    voc = java_keywords_removal(voc)\n",
    "    voc = stop_words_removal(voc)\n",
    "    voc = [w for w in voc if not w.isdigit()]\n",
    "    voc = stemming(voc)\n",
    "    return voc, package\n",
    "\n",
    "def camel_case_split(voc):\n",
    "    new_voc = []\n",
    "    for word in voc:\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', word)\n",
    "        new_voc.extend([m.group(0) for m in matches])\n",
    "    return new_voc\n",
    "\n",
    "def bug_to_corpus(content):\n",
    "    voc = content.split(' ')\n",
    "    voc = tokenize(content.replace('_', ' '))\n",
    "    voc = camel_case_split(voc)\n",
    "    voc = java_keywords_removal(voc)\n",
    "    voc = stop_words_removal(voc)\n",
    "    voc = [w for w in voc if not w.isdigit()]\n",
    "    voc = stemming(voc)\n",
    "    return voc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SKlearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def take(n, iterable):\n",
    "    # Return first n items of the iterable as a list\n",
    "    l = list(islice(iterable, n))\n",
    "    new_dict = {}\n",
    "    for item in l:\n",
    "        new_dict[item[0]] = item[1]\n",
    "    return new_dict\n",
    "\n",
    "def sort_scores(docs_dic, sim_scores):\n",
    "    n = len(docs_dic)\n",
    "    sort_a = sim_scores.argsort()[-n:][::-1]\n",
    "    return sort_a\n",
    "\n",
    "def get_cosine_sim(docs):\n",
    "    vectors = [t for t in get_vectors(docs)]\n",
    "    return cosine_similarity(vectors)\n",
    "\n",
    "def get_vectors(text):\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "\n",
    "def compute_rvsm(chunk_size, bug_voc, code_voc):\n",
    "    bug_str = ' '.join(bug_voc)\n",
    "    docs = [bug_str]\n",
    "    docs_dic = {0: 'self'}\n",
    "    for corpus in code_voc:\n",
    "        if len(corpus.get_voc()) >= chunk_size:\n",
    "            voc = corpus.get_voc()\n",
    "            while len(voc) >= chunk_size:\n",
    "                text = ' '.join(voc[0:chunk_size+1])\n",
    "                voc = voc[chunk_size+1:]\n",
    "                docs.append(text)\n",
    "                docs_dic[len(docs_dic)] = corpus.get_file()\n",
    "        else:\n",
    "            text = ' '.join(corpus.get_voc())\n",
    "            docs.append(text)\n",
    "            docs_dic[len(docs_dic)] = corpus.get_file()\n",
    "\n",
    "    sim_scores = get_cosine_sim(docs)[0]\n",
    "    sorted_index = sort_scores(docs_dic, sim_scores)\n",
    "\n",
    "    rvsm_rank = {}\n",
    "    for i in sorted_index:\n",
    "        if docs_dic[i] in rvsm_rank:\n",
    "            rvsm_rank[docs_dic[i]] = sim_scores[i] if sim_scores[i] > rvsm_rank[docs_dic[i]] else rvsm_rank[docs_dic[i]]\n",
    "            continue\n",
    "        if i == 0:\n",
    "            continue\n",
    "        rvsm_rank[docs_dic[i]] = sim_scores[i]\n",
    "\n",
    "    return rvsm_rank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing for the bug CLI-5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get_voc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/90/229vn9dx6874gdg9vwxw8dq40000gn/T/ipykernel_13961/346908087.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0mcode_corpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcode_to_corpus\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_file_content\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m \u001B[0mcompute_rvsm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbug_corpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcode_corpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/90/229vn9dx6874gdg9vwxw8dq40000gn/T/ipykernel_13961/2235380927.py\u001B[0m in \u001B[0;36mcompute_rvsm\u001B[0;34m(chunk_size, bug_voc, code_voc)\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0mdocs_dic\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'self'\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mcorpus\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcode_voc\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_voc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mchunk_size\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m             \u001B[0mvoc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcorpus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_voc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvoc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mchunk_size\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'get_voc'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def json_file_to_dict(file):\n",
    "    data = {}\n",
    "    with open(file, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    fp.close()\n",
    "    return data\n",
    "\n",
    "def read_file(file):\n",
    "    file_content = \"\"\n",
    "    with open(test_file, 'r') as file:\n",
    "        file_content = file.read()\n",
    "    file.close()\n",
    "    return file_content\n",
    "\n",
    "# Getting the bug report textual content\n",
    "bug_report_content = json_file_to_dict(bug_report_file)\n",
    "title = bug_report_content['title'] if bug_report_content['title'] else \"\"\n",
    "description = bug_report_content['body'] if bug_report_content['body'] else \"\"\n",
    "bug_text_content = title + '\\n' + description + '\\n' + '\\n'.join([comment['body'] for comment in bug_report_content['comments_content'] if 'body' in comment and comment['body']])\n",
    "bug_corpus = bug_to_corpus(bug_text_content)\n",
    "\n",
    "# Getting the test textual content\n",
    "os.chdir(repo_folder)\n",
    "checkout_command = \"git checkout  --quiet \" + buggy_commit\n",
    "os.system(checkout_command)\n",
    "test_file_content = read_file(test_file)\n",
    "code_corpus = code_to_corpus(test_file_content)\n",
    "\n",
    "compute_rvsm(800, bug_corpus, code_corpus) # segment size = 800 tokens in Pathidea"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
