{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Vars declaration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "bug_reports_path = \"/Users/lorenapacheco/Concordia/Masters/bug_mining-2\"\n",
    "defects4j_path = \"/Users/lorenapacheco/Concordia/Masters/defects4j\"\n",
    "output_path = \"/Users/lorenapacheco/Concordia/Masters/BugReportsMining/defects4j/\"\n",
    "\n",
    "stack_trace_regex = r'(?m)^.*?Exception.*(?:\\n+^\\s*at .*)+'\n",
    "logs_regex = r'(ERROR|INFO|WARN|DEBUG|FATAL)\\s+(?P<class>\\w+(\\.\\w+)*)'\n",
    "\n",
    "defects4j_projects_github = {\n",
    "    \"Cli\": \"apache/commons-cli\",\n",
    "    \"Closure\": \"google/closure-compiler\",\n",
    "    \"Codec\": \"apache/commons-codec\",\n",
    "    \"Collections\": \"apache/commons-collections\",\n",
    "    \"Compress\": \"apache/commons-compress\",\n",
    "    \"Csv\": \"apache/commons-csv\",\n",
    "    \"Gson\": \"google/gson\",\n",
    "    \"JacksonCore\": \"FasterXML/jackson-core\",\n",
    "    \"JacksonDatabind\": \"FasterXML/jackson-databind\",\n",
    "    \"Jsoup\": \"jhy/jsoup\",\n",
    "    \"JxPath\": \"apache/commons-jxpath\",\n",
    "    \"Lang\": \"\", # Projects commons-math and common-lang do not have the old commits in github. Will have to use deffects4j zip instead\n",
    "    \"Math\": \"\",\n",
    "    \"Mockito\": \"mockito/mockito\",\n",
    "    \"Time\" : \"JodaOrg/joda-time\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the bug reports with log snippets or stack traces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 bug reports with logs found\n",
      "Collected info added to the file defects4j/bug_reports_with_logs_data\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "bugs_data = {}\n",
    "regex_result = {}\n",
    "\n",
    "def json_file_to_dict(file):\n",
    "    data = {}\n",
    "    with open(os.path.join(file), 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    fp.close()\n",
    "    return data\n",
    "\n",
    "def find_regex(regex, bug_id, text_content):\n",
    "    results = re.finditer(regex, text_content)\n",
    "    if results:\n",
    "        for log in results:\n",
    "            if bug_id not in regex_result.keys():\n",
    "                regex_result[bug_id] = []\n",
    "            regex_result[bug_id].append(log.group())\n",
    "\n",
    "\n",
    "def find_logs_and_stack_traces_txt (bug_id, text_content):\n",
    "    find_regex(logs_regex, bug_id, text_content)\n",
    "    find_regex(stack_trace_regex, bug_id, text_content)\n",
    "\n",
    "def find_logs_and_stack_traces_json (bug_id, bug_report_json):\n",
    "    string_fields_list = [\"summary\", \"description\"]\n",
    "    for field in string_fields_list:\n",
    "        if field in bug_report_json.keys():\n",
    "            find_regex(logs_regex, bug_id, bug_report_json[field])\n",
    "            find_regex(stack_trace_regex, bug_id,  bug_report_json[field])\n",
    "    # going through comments\n",
    "    if \"comments\" in bug_report_json.keys():\n",
    "        for comment in bug_report_json[\"comments\"]:\n",
    "            find_regex(logs_regex, bug_id, comment[\"content\"])\n",
    "            find_regex(stack_trace_regex, bug_id,  comment[\"content\"])\n",
    "\n",
    "def dict_to_json_file(file, dic, folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    with open(os.path.join(folder, file+'.json'), 'w') as fp:\n",
    "        json.dump(dic, fp, sort_keys=True, indent=4)\n",
    "    fp.close()\n",
    "\n",
    "# txt files\n",
    "for file in glob.glob(bug_reports_path +'/*.txt'):\n",
    "    bug_id = os.path.basename(file).replace('.json', '')\n",
    "    with open(file, 'r') as file_obj:\n",
    "        file_content = file_obj.read()\n",
    "    find_logs_and_stack_traces_txt(bug_id, file_content)\n",
    "\n",
    "\n",
    "# json files\n",
    "for file in glob.glob(bug_reports_path +'/*.json'):\n",
    "    bug_id = os.path.basename(file).replace('.json', '')\n",
    "    bug_report_json = json_file_to_dict(file)\n",
    "    find_logs_and_stack_traces_json(bug_id, bug_report_json)\n",
    "\n",
    "print(str(len(regex_result)) + \" bug reports with logs found\")\n",
    "\n",
    "for bug_report_file in regex_result.keys():\n",
    "    bug_report = bug_report_file.split(\".\")[0] #Removing the file extension\n",
    "    project = bug_report.split(\"_\")[0]\n",
    "    bug_id = bug_report.split(\"_\")[1]\n",
    "    if project not in bugs_data.keys():\n",
    "        bugs_data[project]={}\n",
    "    bugs_data[project][bug_id] = {\"log\": regex_result[bug_report_file]}\n",
    "\n",
    "dict_to_json_file(\"bug_reports_with_logs_data\",bugs_data, output_path)\n",
    "print(\"Collected info added to the file defects4j/bug_reports_with_logs_data\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting the commits for each of this bugs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file defects4j/bug_reports_with_logs_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import fnmatch\n",
    "\n",
    "def find_project_folder(project, path):\n",
    "    pattern = fnmatch.translate(project.lower())\n",
    "    results = []\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in dirs:\n",
    "            if fnmatch.fnmatch(name, pattern):\n",
    "                results.append(os.path.join(root, name))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "bugs_data = json_file_to_dict(output_path + \"bug_reports_with_logs_data.json\")\n",
    "for project in bugs_data:\n",
    "    with open(defects4j_path + \"/framework/projects/\" + project + \"/active-bugs.csv\", 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if row[0] in bugs_data[project].keys():\n",
    "                bug_id = row[0]\n",
    "                bugs_data[project][bug_id][\"buggy_commit\"] = row[1]\n",
    "                bugs_data[project][bug_id][\"bugfix_commit\"] = row[2]\n",
    "                bugs_data[project][bug_id][\"bug_report\"] = row[3]\n",
    "\n",
    "dict_to_json_file(\"bug_reports_with_logs_data\",bugs_data, output_path)\n",
    "print(\"Collected info added to the file defects4j/bug_reports_with_logs_data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collecting information about the failing test in each bug"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file defects4j/bug_reports_with_logs_data\n"
     ]
    }
   ],
   "source": [
    "def find_file(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "\n",
    "def read_file_lines(file_name, path):\n",
    "    path = find_file(file_name, path)\n",
    "    if path:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            lines =file.readlines()\n",
    "        file.close()\n",
    "        return lines\n",
    "    return []\n",
    "\n",
    "bugs_data = json_file_to_dict(output_path + \"bug_reports_with_logs_data.json\")\n",
    "for project in bugs_data:\n",
    "    for bug_id in bugs_data[project]:\n",
    "        bugs_data[project][bug_id] [\"defects4j_failing_tests\"] = []\n",
    "        lines = read_file_lines(bug_id, defects4j_path + \"/framework/projects/\" + project + \"/trigger_tests/\")\n",
    "        for line in lines:\n",
    "            if line.startswith(\"--- \"):\n",
    "                failing_test_name = line.replace(\"--- \", \"\").replace(\"\\n\", \"\")\n",
    "                bugs_data[project][bug_id] [\"defects4j_failing_tests\"].append(failing_test_name)\n",
    "\n",
    "dict_to_json_file(\"bug_reports_with_logs_data\",bugs_data, output_path)\n",
    "print(\"Collected info added to the file defects4j/bug_reports_with_logs_data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking if these tests were introduced in the bug_fix commit - projects that have the commits on Github"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file defects4j/bug_reports_with_logs_data\n"
     ]
    }
   ],
   "source": [
    "from github import Github\n",
    "\n",
    "g = Github(\"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\") # TODO: Remove before commiting\n",
    "\n",
    "bugs_data = json_file_to_dict(output_path + \"bug_reports_with_logs_data.json\")\n",
    "\n",
    "for project in bugs_data:\n",
    "    github_repo_id = defects4j_projects_github[project]\n",
    "    if github_repo_id != \"\":\n",
    "        repo = g.get_repo(github_repo_id)\n",
    "        for bug_id in bugs_data[project]:\n",
    "            commit_sha = bugs_data[project][bug_id][\"bugfix_commit\"]\n",
    "            commit = repo.get_commit(commit_sha)\n",
    "            tests_to_be_verified = bugs_data[project][bug_id][\"defects4j_failing_tests\"]\n",
    "            flags_list = []\n",
    "            for test in tests_to_be_verified:\n",
    "                test_file = test.split(\"::\")[0].split(\".\")[-1]\n",
    "                test_case = test.split(\"::\")[1]\n",
    "                # Check if the specific test case was added in the commit\n",
    "                test_added = False\n",
    "                for file in commit.files:\n",
    "                    if file.filename.endswith(test_file + \".java\") and (file.status == \"modified\" or file.status == \"added\"):\n",
    "                        for patch_line in file.patch.split('\\n'):\n",
    "                            if patch_line.startswith(\"+\") and test_case + \"(\" in patch_line:\n",
    "                                test_added = True\n",
    "                                break\n",
    "                flags_list.append(test_added)\n",
    "            bugs_data[project][bug_id][\"flag_failing_tests_added_in_bugfix\"] = all(flags_list)\n",
    "\n",
    "dict_to_json_file(\"bug_reports_with_logs_data\",bugs_data, output_path)\n",
    "print(\"Collected info added to the file defects4j/bug_reports_with_logs_data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking if these tests were introduced in the bug_fix commit - projects that do not have the commits on Github (math and lang)\n",
    "Requires defects4j installed and it is necessary to run the script get_repos.sh to download the defects4j version of these repos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected info added to the file defects4j/bug_reports_with_logs_data\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "projects_list = {\n",
    "    \"Lang\": \"commons-lang.git\",\n",
    "    \"Math\": \"commons-math.git\"\n",
    "}\n",
    "\n",
    "bugs_data = json_file_to_dict(output_path + \"bug_reports_with_logs_data.json\")\n",
    "\n",
    "# Define the working directory where the repositories will be checked out\n",
    "working_dir = \"/tmp/\"\n",
    "\n",
    "# Define the bugs and versions for which to obtain the modified lines\n",
    "for project in projects_list.keys():\n",
    "    folder = projects_list[project]\n",
    "    os.chdir(\"/Users/lorenapacheco/Concordia/Masters/defects4j/project_repos/\" + folder + \"/\") # Moving to defects4j folder\n",
    "    bugs_details = bugs_data[project]\n",
    "    for bug in bugs_details.keys():\n",
    "        commit_sha = bugs_data[project][bug][\"bugfix_commit\"]\n",
    "        output = subprocess.check_output([\"git\", \"show\", commit_sha])\n",
    "        output_str = output.decode(\"utf-8\")\n",
    "        # Split the output into lines\n",
    "        lines = output_str.split(\"\\n\")\n",
    "\n",
    "        # Initialize variables to store file name and diff\n",
    "        filename = None\n",
    "        diffs = {}\n",
    "\n",
    "        # Loop through the lines of the output\n",
    "        for line in lines:\n",
    "            # Check if the line starts with \"diff --git a/\"\n",
    "            if line.startswith(\"diff --git a/\"):\n",
    "                # If yes, extract the file name\n",
    "                filename = line[13:]\n",
    "                # Reset the diff for the new file\n",
    "                diffs[filename] = \"\"\n",
    "            # Check if the line starts with \"+++\" or \"---\"\n",
    "            elif line.startswith(\"+++\") or line.startswith(\"---\"):\n",
    "                # If yes, skip the line\n",
    "                continue\n",
    "            # Check if the line starts with \"+\"\n",
    "            elif line.startswith(\"+\") and filename:\n",
    "                # If yes, append the line to the diff\n",
    "                diffs[filename] += line + \"\\n\"\n",
    "            # Check if the line starts with \"-\"\n",
    "            elif line.startswith(\"-\"):\n",
    "                # If yes, append the line to the diff\n",
    "                diffs[filename] += line + \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "        tests_to_be_verified = bugs_data[project][bug][\"defects4j_failing_tests\"]\n",
    "        flags_list = []\n",
    "        for test in tests_to_be_verified:\n",
    "            test_file = test.split(\"::\")[0].split(\".\")[-1]\n",
    "            test_case = test.split(\"::\")[1]\n",
    "            # Check if the specific test case was added in the commit\n",
    "            test_added = False\n",
    "            for file in diffs.keys():\n",
    "                if file.endswith(test_file + \".java\"):\n",
    "                    for patch_line in diffs[file].split('\\n'):\n",
    "                        if patch_line.startswith(\"+\") and test_case + \"(\" in patch_line:\n",
    "                            test_added = True\n",
    "                            break\n",
    "            flags_list.append(test_added)\n",
    "        bugs_data[project][bug][\"flag_failing_tests_added_in_bugfix\"] = all(flags_list)\n",
    "\n",
    "dict_to_json_file(\"bug_reports_with_logs_data\",bugs_data, output_path)\n",
    "print(\"Collected info added to the file defects4j/bug_reports_with_logs_data\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
